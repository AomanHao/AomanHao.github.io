---
title: 机器学习_分类_SVM
date: 2018-07-16 09:39:40
tags: [GitHub, Mysql]
---


机器学习_分类_SVM

<!--more-->

---
支持向量机（Support Vector Machine, SVM）的基本模型是在特征空间上找到最佳的`分离超平面`使得训练集上正负样本间隔最大。

二分类问题的有监督学习算法，引入了核方法之后SVM也可以用来解决非线性问题
一般SVM有下面三种：

>1、硬间隔支持向量机（线性可分支持向量机）：当训练数据线性可分时，可通过硬间隔最大化学得一个线性可分支持向量机。<br>
2、软间隔支持向量机：当训练数据近似线性可分时，可通过软间隔最大化学得一个线性支持向量机。<br>
3、非线性支持向量机：当训练数据线性不可分时，可通过核方法以及软间隔最大化学得一个非线性支持向量机。


![SVM](https://pic2.zhimg.com/80/v2-d2b03cf98869849d1d6a4d91a05d6571_hd.jpg)

SVM算法认为图1中的分类器A在性能上优于分类器B，其依据是A的分类间隔比B要大
>这两条平行虚线正中间的分界线就是在保持当前决策面方向不变的前提下的最优决策面。两条虚线之间的垂直距离就是这个最优决策面对应的`分类间隔`。<br>

>那个具有“最大间隔”的决策面就是SVM要寻找的最优解,而这个真正的最优解对应的两侧虚线所穿过的样本点，就是SVM中的支持样本点，称为`支持向量`。<br>

>对于图1中的数据，A决策面就是SVM寻找的最优解，而相应的三个位于虚线上的样本点在坐标系中对应的向量就叫做`支持向量`。



### 基于最大间隔分割数据
优点，错误率低，计算开销不大，结果容易解释
缺点，对参数调节敏感，原始分类器不加修改只能解决二类问题。

输人数据给分类器会输出一个类别标签,单位阶跃函数）的函数对$$ w^{t}x+b $$作用得到$$ f(w^{t}x+b) $$,其中当u<0时输出-1, 反之则输出+1。这是由于-1和+1仅仅相差一个符号，方便数学上的处理。

如果数据点处于正方向（即+1类 ）并且离`分隔超平
面`很远的位置时，$$ w^{t}x+b $$会是一个很大的正数，同时$$ label*(w^{t}x+b) $$也会是一个很大的正数。而如果数据点处于负方向（-1类 ）并且离`分隔超平面`很远的位置时，此时由于类别标签为-1，则$$ label*(w^{t}x+b) $$仍然是一个很大的正数。
