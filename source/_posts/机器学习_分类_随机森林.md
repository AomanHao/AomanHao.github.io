---
title: 机器学习_分类_随机森林
date: 2018-07-16 09:39:40
tags: [GitHub, Mysql]
---


机器学习_分类_随机森林

<!--more-->
决策树算法是借助于树的分支结构实现分类。

叶子节点：存放决策结果
非叶子节点：特征属性，及其对应输出，按照输出选择分支
决策过程：从根节点出发，根据数据的各个属性，计算结果，选择对应的输出分支，直到到达叶子节点，得到结果


决策树使用自顶向下递归分治法，并采用不回溯的贪心策略。分裂属性的选择算法很多，这里介绍3种常用的算法：信息增益（Information gain）、增益比率（gain ratio）、基尼指数（Gini index）。<br>
>我们通过**基尼不纯度**或者**熵**来对一个集合进行的有序程度进行量化，然后引入**信息增益**概念对一次拆分进行量化评价

### 基尼不纯度
基尼不纯度是指将来自集合中的某种结果随机应用于集合中某一数据项的预期误差率。该值越高，说明拆分的越不理想，如果该值为 0，说明完美拆分。

$$ Gini(D)=1−∑_i=(1^m)p_i^2 $$

### 熵
用来表示集合的无序程度，熵越大表示集合越混乱<br>
$$E = -P * log2P $$

>基尼不纯度与熵对比<br>
两者主要区别在于，熵到达峰值的过程相对慢一些。因此熵对混乱集合的「判罚」往往更重一些。通常情况下，熵的使用更加频繁。

### 信息增益（Information Gain） 
基于香浓的信息论，**信息熵**表示不确定度，均匀分布时，不确定度最大，此时熵就最大。<br>当选择某个特征对数据集进行分类时，数据集分类后的信息熵会比分类前的小，其差值即为信息增益。<br>**信息增益**可以衡量某个特征对分类结果的影响大小，**越大越好**。
>信息增益=abs(信息熵（分类后）-信息熵（分类前）)
$$ Gain(R)=Info(D)−InfoR(D) $$

### 决策树降剪枝

>为什么要剪枝
训练出得决策树存在过度拟合现象——决策树过于针对训练的数据，专门针对训练集创建出来的分支，其熵值可能会比真实情况有所降低。

>如何剪枝
人工设置一个信息增益的阀值，自下而上遍历决策树，将信息增益低于该阀值的拆分进行合并

### 处理缺失数据
>决策树模型还有一个很大的优势，就是可以容忍缺失数据。如果决策树中某个条件缺失，可以按一定的权重分配继续往以后的分支走，最终的结果可能有多个，每个结果又一定的概率，即：
```
最终结果=某个分支的结果 x 该分支的权重(该分支下的结果数/总结果数)
```

---

###生成算法：ID3和C4.5。
#### 1、ID3算法
ID3算法可用于划分标称型数据集，没有剪枝的过程，为了去除过度数据匹配的问题，可通过裁剪合并相邻的无法产生大量信息增益的叶子节点（例如设置信息增益阀值）。
考虑某个特征后，信息熵减小的多，这个特征就是好的特征(在每层分裂时，选择使得Gain(R)最大的属性作为分裂属性)
ID3算法中根据信息增益评估和选择特征，每次选择信息增益最大的特征作为判断模块建立子结点
>缺点：1、此公式偏向数据量多的属性，如果样本分布不均，则会导致过拟合。<br>2、不能处理连续分布的数据特征

#### 2、C4.5算法
C4.5算法用**信息增益率**来选择属性，继承了ID3算法的优点
优点：
>1、克服了用信息增益选择属性时偏向选择取值多的属性的不足；<br>2、在树构造过程中进行剪枝；<br>3、能够完成对连续属性的离散化处理；<br >4、能够对不完整数据进行处理。
---

