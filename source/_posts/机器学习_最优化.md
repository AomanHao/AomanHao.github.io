---
title: 机器学习_最优化
date: 2018-07-16 09:39:40
tags: [机器学习, 最优化]
---

机器学习_最优化

<!--more-->

铺垫：<br>
微分意义<br>
```
1、函数图像中，某点的切线的斜率
2、函数的变化率
```
梯度意义<br>
>梯度就是分别对每个变量进行微分，然后用逗号分割开，梯度是用<>包括起来，说明梯度其实一个向量。
```
1、在单变量的函数中，梯度其实就是函数的微分，代表着函数在某个给定点的切线的斜率
2、在多变量函数中，梯度是一个向量，向量有方向，梯度的方向就指出了函数在给定点的上升最快的方向
```
>梯度的方向实际就是函数在此点上升最快的方向！而我们需要朝着下降最快的方向走，自然就是负的梯度的方向，所以此处需要加上负号

### 梯度下降法（Gradient Descent）

>梯度下降法的优化思想是用当前位置负梯度方向作为搜索方向，因为该方向为当前位置的最快下降方向，所以也被称为是”最速下降法“。最速下降法越接近目标值，步长越小，前进越慢

梯度下降法的缺点：
```
（1）靠近极小值时收敛速度减慢，；

（2）直线搜索时可能会产生一些问题；

（3）可能会“之字形”地下降。
```
$$\theta^1=\theta^0 - \alpha\nabla(j(\theta)$$
是关于Θ的一个函数，我们当前所处的位置为Θ0点，要从这个点走到J的最小值点
$\nabla$ 是梯度,$\alpha$是学习率或者步长

#### 批量梯度下降法
将$j(\theta)$对$\theta$求偏导，得到每个$\theta$对应的的梯度：每个参数$\theta$的梯度负方向，来更新每个$\theta$
>优点：它得到的是一个全局最优解
缺点：数据量大，计算缓慢
#### 随机梯度下降
随机梯度下降是通过每个样本来迭代更新一次，如果样本量很大的情况（例如几十万），那么可能只用其中几万条或者几千条的样本
>优点：只用部分数据继续优化，运算量小
缺点：损失一部分进度，增加迭代次数

两者关系：
>随机梯度下降方法以损失很小的一部分精确度和增加一定数量的迭代次数为代价，换取了总体的优化效率的提升。增加的迭代次数远远小于样本的数量。

总结：
>*批量梯度下降*---最小化所有训练样本的损失函数，使得最终求解的是**全局的最优解**，即求解的参数是使得风险函数最小，但是对于大规模样本问题效率低下。

>*随机梯度下降*---最小化每条样本的损失函数，虽然不是每次迭代得到的损失函数都向着全局最优方向， 但是大的整体的方向是向全局最优解的，最终的结果往往是在全局最优解附近，适用于**大规模训练样本**情况。
---

### 牛顿法和拟牛顿法
牛顿法是一种在实数域和复数域上近似求解方程的方法。牛顿法最大的特点就在于它的**收敛速度很快**。

例如：方法使用函数$f(x)$的泰勒级数的前面几项来寻找方程$f(x)= 0$的根。

1、选择一个接近函数$ f (x)$零点的 $x_0$，计算相应的$ f (x_0)$ 和切线斜率$f  ' (x_0)$（这里$f ' $表示函数$ f $ 的导数）。然后我们计算穿过点$(x_0,  f  (x_0))$ 并且斜率为$f '(x_0)$的直线和 $x $轴的交点的$x$坐标，也就是求如下方程的解：
$$x*f'(x_0)+f(x_0)-x_0*f'(x_0)=0$$
求得新的$x$坐标$x_1$,$x_1$比$x_0$更加接近收敛值的解，也就是使得$f(x)=0$，迭代公式：
$$x_n+1=x_n-f(x_n)/f'(x_n)$$

>如果$f '$ 是连续的，牛顿法必定收敛

总结：
>牛顿法的优缺点
优点：二阶收敛，收敛速度快；
缺点：牛顿法是一种迭代算法，每一步都需要求解目标函数的Hessian矩阵的逆矩阵，计算比较复杂。

###<font color=red>牛顿法是二阶收敛，梯度下降是一阶收敛，所以牛顿法就更快</font>
>牛顿法就是用一个二次曲面去拟合你当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的最优下降路径。

### 拟牛顿法（Quasi-Newton Methods）
拟牛顿法是求解非线性优化问题最有效的方法之一。

拟牛顿法的本质思想是改善牛顿法每次需要求解复杂的Hessian矩阵的逆矩阵的缺陷，它使用<font color=red>正定矩阵来近似Hessian矩阵的逆</font>，从而简化了运算的复杂度。



[参考文章](https://www.cnblogs.com/shixiangwan/p/7532830.html)