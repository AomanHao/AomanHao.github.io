---
title: 面经—机器学习
date: 2018-07-16 09:39:40
tags: [C++, 机器学习, 笔面试]
---

面经—机器学习

<!--more-->

## CVTE面经
作者：一一后
链接：https://www.nowcoder.com/discuss/88069
来源：牛客网

1.解释方差

2.PCA的实现过程；推导PCA

3.传统的图像特征有哪些

4.Sift特征为什么能实现尺度不变性（讲sift原理到一半，我发现完全解释不了为啥尺度不变，就停了，尴尬）
[参考](https://blog.csdn.net/u014485485/article/details/78681086?locationNum=1&fps=1）
```
尺度不变性：
不管原图尺度是多少，在包含了所有尺度的尺度空间下都能找到那些稳定的极值点，这样就做到了尺度不变！
高斯函数是唯一可行的尺度空间核
```


5.Hough直线检测的原理

6.梯度下降和牛顿法的区别
```
牛顿法的优缺点
优点：二阶收敛，收敛速度快；
缺点：牛顿法是一种迭代算法，每一步都需要求解目标函数的Hessian矩阵的逆矩阵，计算比较复杂。

梯度下降法的优化思想是用当前位置负梯度方向作为搜索方向，因为该方向为当前位置的最快下降方向，所以也被称为是”最速下降法“。最速下降法越接近目标值，步长越小，前进越慢
梯度下降法的缺点：
靠近极小值时收敛速度减慢，；
直线搜索时可能会产生一些问题；
可能会“之字形”地下降。

牛顿法是二阶收敛，梯度下降是一阶收敛，所以牛顿法就更快
```
7.SVM和Lr的共同点和不同点
```
LR和SVM都是分类算法
LR和SVM都是线性分类算法
LR和SVM都是监督学习算法
LR和SVM都是判别模型
LR和SVM在学术界和工业界都广为人知并且应用广泛

不同：
损失函数
LR：逻辑回归方法基于概率理论
逻辑回归考虑全局（远离的点对边界线的确定也起作用）
对数据不做处理
LR必须另外在损失函数上添加正则项

SVM：几何间隔最大化原理
支持向量机只考虑局部的边界线附近的点，线性SVM不直接依赖于数据分布
线性SVM依赖数据表达的距离测度，所以需要对数据先做归一化
SVM的损失函数就自带正则
```
8.rf和Adaboost的异同(优秀的基于决策树的组合算法)
```
1，相同：二者都是bootsrap自助法选取样本。 
2，相同：二者都是要训练很多棵决策树。 
3，不同：adaboost后面树的训练，其在变量抽样选取的时候，对于上一棵树分错的样本，抽中的概率会加大。 
4，不同：随机森林在训练每一棵树的时候，随机挑选了部分变量作为拆分变量，而不是所有的变量都去作为拆分变量。 
5，不同：在预测新数据时，adaboost中所有的树加权投票来决定因变量的预测值，每棵树的权重和错误率有关；随机森林按照所有树中少数服从多数树的分类值来决定因变量的预测值。
```
9.给出一堆大小不一的矩形框，快速求矩形框的灰度值之和（当时没理解，这不是肯定要遍历么…后来结束后我想这些矩形框可能是重叠的，估计是要问我关于Bing中快速求梯度的算法）

10.有什么要问他的

---
### 作业帮提前批机器学习算法岗
作者：编程一头牛
链接：https://www.nowcoder.com/discuss/90245
来源：牛客网

对数据预处理怎么填充的缺失值，哪些判定为异常值，对连续属性进行离散化有什么好处，Logistic回归能处理浮点数吗？多项式组合特征对哪个模型中效果提升最大。这个没答上来，问了面试官，面试官说是Logistic回归里面提升最大，而且组合起来的两个特征也是想出来的，没什么理论支撑。还问了模型的评价指标AUC是如何计算的，ROC曲线的横纵坐标代表了什么含义。XGBoost模型里面参数有哪些？如何发现过拟合。XGBoost模型中对数据进行采样的好处？

如何最快的找出两个集合中的交集，提出用哈希表的方法，问了这种方法的复杂度，然后又问如果这两个集合都特别大，不能再内存中构建哈希表该如何做？可能是想让我回答多线程相关的内容，但是我不会。如何设计哈希表？期间也问过有编过多线程多进程的代码吗

---
### 深信服【机器学习】一面
作者：Rnanprince
链接：https://www.nowcoder.com/discuss/87283
来源：牛客网

【机器学习】一面：
1.项目介绍，研究的最成功的地方，我以写的文章为例，涉及到的知识点就问
2.笔试的数组求和100怎么做的？
没抽到这个题，但是做过；接着我说做了查找重复字符串最大长度，深搜，过了就没想别的方法
一个数组，求最长的连续子序列的起始下标，当时没理解明白，其实有歧义，简单说了一下
3.自己的哪些方面的优点没有涉及到，介绍一下
提到了SVM和决策树，介绍一下什么情况下使用？
svm:
```
这个模型的优势是什么？

分类效果好；
可以有效地处理高维空间的数据；
可以有效地处理变量个数大于样本个数的数据；
只是使用了一部分子集来进行训练模型，所以SVM模型不需要太大的内存；
可以提高泛化能力；
无局部极小值问题；
他什么情况下表现最好？

数据的维度较高；
需要模型具有非常强的泛化能力；
样本数据量较小时；
解决非线性问题；
这个模型的缺点是什么？

无法处理大规模的数据集，因为该算法需要较长的训练时间；
无法有效地处理包含噪声太多的数据集；
SVM模型没有直接给出概率的估计值，而是利用交叉验证的方式估计，这种方式耗时较长；
对缺失数据非常敏感；
对于非线性问题，有时很难找到一个合适的核函数。
什么条件下它表现很差？

数据集的数据量过大；
数据集中的含有噪声；
数据集中的缺失较多的数据；
对算法的训练效率要求较高；
根据我们当前数据集的特点，为什么这个模型适合这个问题。 
该项目所提供的样本数据相对较少；
该问题是属于非线性问题；
数据集经过“独热编码”后，维度较高
```

决策树：
```
这个模型的优势是什么？

决策树易于实现和理解；
对于决策树，数据的准备工作一般比较简单；
能够同时处理多种数据类型
给定一个决策树模型，可以根据产生的决策树推出相应的逻辑表达式；
通过静态测试来对模型的表现进行评价；
在相对较短的时间内可以对大量的数据做出非常好的结果；
决策树可以很好地扩展到大型数据中，同时决策树的大小独立于数据库的大小；
计算复杂度相对较低，结果的输出易于理解，对部分的数据缺失不敏感。
他什么情况下表现最好？

实例是由“属性-值”对表示的；
目标函数具有离散的输出值；
训练数据集包含部分错误(决策树对错误有适应性)；
训练数据缺少少量属性的实例。
这个模型的缺点是什么？

易于出现过拟合问题；
忽略了数据集中属性之间的相关性；
对于类比不一致的样本，决策树的信息增益倾向于那些数据值较多的特征
什么条件下它表现很差？

决策树匹配的数据过多时；
分类的类别过于复杂；
数据的属性之间具有非常强的关联。
根据我们当前数据集的特点，为什么这个模型适合这个问题。

不需要准备太多的训练数据，不需要对数据过多的处理如删除空白值等；
易于编码；
该问题是非线性问题，决策树能够很好地解决非线性问题；
算法的执行效率高，对机器的要求较小。
```


---
### 360浏览器事业部 推荐算法工程师
作者：泡了个泡
链接：https://www.nowcoder.com/discuss/77924
来源：牛客网

二面

1.项目

2.SVM原始问题为什么要转化为对偶问题，为什么对偶问题就好求解，原始问题不能求解么

3.K-means 中我想聚成100类 结果发现只能聚成98类，为什么

4.进程中的内存分段是怎样的

5.每个线程有哪些东西是自己独享的

6.一枚不均匀的硬币，我抛了100次，有70次朝上，那么第101次朝上的概率是多少

这个概率怎么样，公示是如何推导出来的

7.给你个字符串，字符串是个数字，怎么转换为int型，不用库函数的话

8.4个海盗，100个金币，每个人轮流提方案，如果你的方案有半数以上通过，那么久可以，否则就会被杀掉，如果你是第一个人，那么你怎么提方案比较好

9.你的优点是什么

### 腾讯沈阳现场一面

1.项目

2.特征选择方法都有用过哪些

3.随机森林怎么进行特征选择

4.用过哪些机器学习算法

5.加密方法知道哪些

6.MD5可逆么

7.word2vec用过么

8.极大似然估计是什么意思

9.上过哪些课

10.排序算法哪些时间复杂度比较低

11.计算机网络了解多少

### 阿里 新零售 天猫 算法工程师-机器学习
一面
先是一个简单的自我介绍；
1.然后介绍了项目的框架和主要创新点；

2.说一下随机森林和Adaboost，以及区别

3.说一下GBDT和Adaboost，以及区别

4.说一下LDA的原理

5.对于PCA，会有第一主成分、第二主成分，怎么为什么第一主成分是第一，原因是什么？

6.PCA的主成分是怎么得到的

3.面向对象的三要素

4.对深度学习了解多少

5.你觉得深度学习的方法和传统机器学习比，有什么大的优势

----

### 腾讯提前批
作者：IamBright
链接：https://www.nowcoder.com/discuss/75166
来源：牛客网

女朋友在广州又不想换工作的情况下，微信的机器学习算法工程师是最适合我实习的岗位了，因此最先让腾讯的同学内推了一波，在基本没有准备的情况下，接到了提前批电话一面二面，毫无意外的挂了。

电话一面
聊论文，但多数听我在说，没插话什么问题。最后问我第二篇论文里RNN实现的时候有什么trick。
问了问凸优化了解吗？传统机器学习了解吗？我答机器学习基本知识都学过，凸优化只了解和机器学习优化算法相关的。也没有继续问细节了。
编程题：打印所有子集，我用了迭代，但是写的比较蠢，好在不用调试运行
电话二面
聊论文，最后问了我跟什么算法做了对比，问我研究的实际意义，产业界现在的水平
编程题：打印螺旋矩阵，要我给一个可运行的结果。很简单的题，我一个符号错误调了很久都没发现，这里应该就印象很差了。
linux里查看端口被占用的命令，linux不熟，没答上。
AUC是什么？我说了是ROC曲线下面积，但是想不起来ROC是啥。我都是做序列数据，没做过二分类问题。
LR和SVM的区别。我说了损失函数不同，然后说了SVM通过核技巧可以更好的应对非线性，但是前面好差，这里也没好好组织语言了。
提前批挂的没什么话说，就是没准备，好久没做过算法题的情况下，突然出题做就很不顺手。而且机器学习的基础知识都有点忘了，像AUC这种没用过的，基本一问就懵逼。

之后跟工作的同学聊了一下，来牛客刷了刷面经，制定了简单的复习内容和刷题计划。花了一周时间，复习了一下西瓜书前11章和deep learning book前11章，刷了leetcode上三四十道medium的题吧（链表、字符串、迭代、dfs、堆、树、动态规划等每天刷一类题练练手），并且给自己做完2篇论文都准备了面试介绍版，又让同学推了阿里和网易，并进入腾讯笔试流程。