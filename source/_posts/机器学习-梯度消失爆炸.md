---
title: 机器学习-梯度消失爆炸
date: 2018-08-16 09:39:40
tags: [机器学习]
toc: true
---

机器学习-梯度消失爆炸

<!--more-->
### 梯度消失
本层的神经元的激活等于上一层神经元对应的权值进行加权和运算，
最后通过一个非线性函数（激活函数）如ReLu，sigmoid等函数，
最后得到的结果就是本层神经元的输出，
逐层逐神经元通过该操作向前传播，最终得到输出层的结果。 

梯度消失的影响：
1) 浅层基本不学习，后面几层一直在学习，失去深度的意义。
2) 无法收敛。

梯度消失的现象呢？因为通常神经网络所用的激活函数是sigmoid函数
这个函数有个特点:<br>
>就是能将负无穷到正无穷的数映射到0和1之间，并且对这个函数求导的结果是f′(x)=f(x)(1−f(x))。<br>因此两个0到1之间的数相乘，得到的结果就会变得很小了。<br>神经网络的反向传播是逐层对函数偏导相乘，因此当神经网络层数非常深的时候<br>最后一层产生的偏差就因为乘了很多的小于1的数而越来越小，最终就会变为0，从而导致层数比较浅的权重没有更新

>一是在深层网络中，网络层数过多二是采用了不合适的损失函数，比如sigmoid

### 梯度爆炸

就是由于初始化权值过大，前面层会比后面层变化的更快，就会导致权值越来越大，梯度爆炸的现象就发生了。

### 解决
用ReLU激活函数来替代sigmoid函数。 
>区别：（1）sigmoid函数值在[0,1],ReLU函数值在[0,+无穷]，所以sigmoid函数可以描述概率，ReLU适合用来描述实数；（2）sigmoid函数的梯度随着x的增大或减小和消失，而ReLU不会。

早期多层神经网络如果用sigmoid函数或者hyperbolic tangent作为激活函数，如果不进行pre-training的话，会因为gradient vanishing problem而无法收敛。 

而预训练的用处：规则化，防止过拟合；压缩数据，去除冗余；强化特征，减小误差；加快收敛速度。而采用ReLu则不需要进行pre-training。