---
title: 机器学习_最优化_损失函数
date: 2018-07-22 09:39:40
tags: [机器学习, 最优化]
toc: true
---

机器学习_最优化_损失函数

<!--more-->
通常机器学习每一个算法中都会有一个目标函数，算法的求解过程是通过对这个目标函数优化的过程。

在分类或者回归问题中，通常使用**损失函数（代价函数）**作为其目标函数。损失函数用来评价模型的预测值和真实值不一样的程度，损失函数越好，通常模型的性能越好。不同的算法使用的损失函数不一样。 
>损失函数分为经验风险损失函数和结构风险损失函数

|损失函数|概念|
|--|--|
|经验风险损失函数|预测结果和实际结果的差别|
|结构风险损失函数|经验风险损失函数+正则项|

$$ θ^∗=argmin(1/N)\sum_i=1^n\L(y_i,f(x_i;θ_i))\+λ\phi(\theta) $$


### 0-1损失函数和绝对值损失函数 
原理：预测值和目标值不相等为1，否则为0。<br>绝对值损失函数为：<br>
$$ L(Y,f(X))=1, if Y≠f(X)$$
$$ L(Y,f(X))=0, if Y=f(X)$$

感知机就是用的这种损失函数<br>
改进：Y-f(X)<阈值T判断相等
绝对值损失函数为： 
$$ L(Y,f(X)=|Y−f(X)| $$


### log对数损失函数
原理：假设样本服从伯努利分布（0-1）分布，然后求得满足该分布的似然函数，接着用对数求极值。<br>
log损失函数的标准形式： <br>
$$ L(Y,P(Y|X))=−logP(Y|X) $$


### 平方损失函数 
最小二乘法是线性回归的一种方法，它将回归的问题转化为了凸优化的问题。最小二乘法的基本原则是：最优拟合曲线应该使得所有点到回归直线的距离和最小。通常用欧几里得距离进行距离的度量。<br>平方损失的损失函数为： <br>
$$ L(Y|f(X))=∑N(Y−f(X))^2 $$



### 指数损失函数 
AdaBoost就是一指数损失函数为损失函数的。 
指数损失函数的标准形式： <br>
$$ L(Y|f(X))=exp[−yf(x)]  $$


### Hinge损失函数 
Hinge损失函数和SVM是息息相关的。在线性支持向量机中，最优化问题可以等价于<br> 
$$ 1/m\sum_i=1^m\l(wx_i+by_i)+||w||^2 $$<br>
其中$$ l(wx_i+by_i) $$ 就是hinge损失函数，后面相当于L2正则项。 

Hinge函数的标准形式： <br>
$$ L(y)=max(0,1−ty) $$<br>
>y的值在-1和+1之间就可以了,使分类器可以更专注于整体的分类误差



[参考文章](https://blog.csdn.net/weixin_37933986/article/details/68488339)