---
title: 机器学习-正则化-L1L2
date: 2018-07-19 09:39:40
tags: [机器学习]
---

机器学习-正则化-L1L2

<!--more-->


---
样本数据量大：经验⻛风险最⼩小化

样本数据量小：结构⻛风险最⼩小化==正则化

---

经验风险最⼩小化（empirical risk minimization）认为经验⻛风险最⼩小的模型是最优的模型，即求解最优化问题
$$ minf ∈ F(1/N)\sum_{i=1}^NL(y_i,f(x_i))$$

样本容量量⾜足够⼤大的时候，经验⻛风险最⼩小化学习效果良好

---
结构⻛风险=经验⻛风险+模型复杂度的正则化项（regularizer）或罚项（penalty term）
$$ minf ∈ F(1/N)\sum_{i=1}^NL(y_i,f(x_i))+\lambda{J(f)}$$

$J(f)$是模型的复杂度，模型$f$越复杂，复杂度$J(f)$越大。
$\lambda ≥ 0$是系数，⽤用以权衡经验⻛风险和模型复杂度。
>结构⻛风险⼩需要1、经验⻛风险和2、模型复杂度同时⼩

---
### 范数
因为非负性：可以做损失函数，正则项
>损失函数通常是⼀个有下确界的函数

常用范数：
L0

L1:绝对值
$$||x||=\sum_{i=1}^{d}{|x_i|}$$

L2；平方再开根号
$$||x||_2=(\sum_{i=1}^{d}{|x_i^2|})^{1/2}$$

Lp
$$||x||_2=(\sum_{i=1}^{d}{|x_i^p|})^{1/p}$$

p=1,曼哈顿距离，L1范数，表示某个向量量中所有元素绝对值的和<br>
p=2,欧式距离，L2范数

使用L1正则项，倾向于使参数稀疏化，使用L2正则项，使参数稠密的接近于0。

---

>正则项是为了降低模型的复杂度，从而避免模型区过分拟合训练数据，包括噪声与异常点（outliers）。从另一个角度上来讲，正则化即是假设模型参数服从先验概率，即为模型参数添加先验，只是不同的正则化方式的先验分布是不一样的。这样就规定了参数的分布，使得模型的复杂度降低（试想一下，限定条件多了，是不是模型的复杂度降低了呢），这样模型对于噪声与异常点的抗干扰性的能力增强，从而提高模型的泛化能力。还有个解释便是，从贝叶斯学派来看：加了先验，在数据少的时候，先验知识可以防止过拟合；从频率学派来看：正则项限定了参数的取值，从而提高了模型的稳定性，而稳定性强的模型不会过拟合，即控制模型空间。
